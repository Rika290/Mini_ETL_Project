# Mini ETL Pipeline using Airflow, Pandas & Snowflake

## Project Overview:
- This project demonstrates a Mini Data Engineering Pipeline where data is extracted from an API, transformed using Pandas, orchestrated with Apache Airflow, and loaded into Snowflake.
- The goal was to practice building an industry-style ETL pipeline with scheduling, reproducibility, and cloud data warehouse integration.

## Key Highlights:
- Extract â†’ Pulled random user data from RandomUser API.
- Transform â†’ Cleaned & reshaped the data with Pandas (renamed columns, dropped unnecessary fields, converted datatypes).
- Load â†’ Inserted the transformed dataset into Snowflake using SnowflakeHook.
- Orchestration â†’ Built a DAG in Apache Airflow with PythonOperator for task scheduling and reproducibility.
- Version Control â†’ Tracked changes with Git & GitHub.

## Tech Stack:
- Python (Pandas, Requests) â€“ Data extraction & transformation
- Apache Airflow â€“ Workflow orchestration
- Snowflake â€“ Cloud data warehouse
- Git & GitHub â€“ Version control

## ðŸ“‚ Project Structure
- `README.md` â†’ documentation
- `project_1_Mini ETL.py` â†’ airflow DAG (ETL pipeline code)
- `api_data.csv` â†’ extracted raw API data
- `transformed_api_data.csv` â†’ transformed & cleaned data
- `project_1_DAG_run.png` â†’ screenshot of Airflow DAG execution
  
## Workflow Architecture:
ETL Flow:
Extract (API) â†’ Transform (Pandas) â†’ Load (Snowflake)
