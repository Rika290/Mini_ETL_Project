Mini ETL Pipeline using Airflow, Pandas & Snowflake

Project Overview:
- This project demonstrates a Mini Data Engineering Pipeline where data is extracted from an API, transformed using Pandas, orchestrated with Apache Airflow, and loaded into Snowflake.
- The goal was to practice building an industry-style ETL pipeline with scheduling, reproducibility, and cloud data warehouse integration.

Key Highlights:
- Extract â†’ Pulled random user data from RandomUser API.
- Transform â†’ Cleaned & reshaped the data with Pandas (renamed columns, dropped unnecessary fields, converted datatypes).
- Load â†’ Inserted the transformed dataset into Snowflake using SnowflakeHook.
- Orchestration â†’ Built a DAG in Apache Airflow with PythonOperator for task scheduling and reproducibility.
- Version Control â†’ Tracked changes with Git & GitHub.

Tech Stack:
- Python (Pandas, Requests) â€“ Data extraction & transformation
- Apache Airflow â€“ Workflow orchestration
- Snowflake â€“ Cloud data warehouse
- Git & GitHub â€“ Version control

ðŸ“‚ Project Structure
â”œâ”€â”€ README.md                # Project documentation
â”œâ”€â”€ project_1_Mini ETL.py    # Airflow DAG (ETL pipeline code)
â”œâ”€â”€ api_data.csv             # Extracted raw API data
â”œâ”€â”€ transformed_api_data.csv # Transformed & cleaned data
â”œâ”€â”€ project_1_DAG_run.png    # Screenshot of Airflow DAG execution

Workflow Architecture:
ETL Flow:
Extract (API) â†’ Transform (Pandas) â†’ Load (Snowflake)
